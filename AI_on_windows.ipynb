{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model\n",
    "import cv2\n",
    "import cvlib as cv\n",
    "from mtcnn import MTCNN\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, face_pixels):\n",
    "\timport numpy as np\n",
    "\tfrom sklearn.preprocessing import Normalizer\n",
    "\tface_pixels = face_pixels.astype('float32')\n",
    "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
    "\tface_pixels = (face_pixels - mean) / std\n",
    "\tsamples = np.expand_dims(face_pixels, axis=0)\n",
    "\tyhat = model.predict(samples)\n",
    "\treturn yhat[0]\n",
    "\n",
    "class AI_Retraining:\n",
    "\tdef __init__(self, rotation_range = 60, horizontal_flip = True):\n",
    "\t\tfrom mtcnn import MTCNN\n",
    "\t\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\t\tfrom keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\t\tself.datagen = ImageDataGenerator(rotation_range= rotation_range, horizontal_flip= horizontal_flip)\n",
    "\t\tself.face_detector = MTCNN()\n",
    "\t\tself.encoder = LabelEncoder()\n",
    "\t\tself.X_image = []\n",
    "\t\tself.y_label = []\n",
    "\t\tself.size = (160,160)\n",
    "\tdef load_image(self, path):\n",
    "\t\timport cv2\n",
    "\t\timport glob\n",
    "\t\ttry:\n",
    "\t\t\tself.path = path\n",
    "\t\t\tfile_path = self.path\n",
    "\t\t\tfor dirpath, dirnames, filenames in os.walk(file_path):\n",
    "\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\ti = file_path + \"/\" + filename\n",
    "\t\t\t\t\timg = cv2.imread(i)\n",
    "\t\t\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\t\t\t\t\tself.X_image.append(img)\n",
    "\t\t\t\t\tlabel = i.split(\"/\")[-1]\n",
    "\t\t\t\t\t#label = label.split(\"//\")[-1]\n",
    "\t\t\t\t\tlabel = label[:len(label)-4]\n",
    "\t\t\t\t\tself.y_label.append(label)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Invalid Path\")\n",
    "\tdef extract_face(self):\n",
    "\t\timport cv2\n",
    "\t\timport os\n",
    "\t\timport pandas as pd\n",
    "\t\tX=[]\n",
    "\t\ty=[]\n",
    "\t\ttry:\n",
    "\t\t\tif(os.path.isdir(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Extracted Face\") == True):\n",
    "\t\t\t\tfor dirpath, dirnames, filenames in os.walk(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Extracted Face\"):\n",
    "\t\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\t\ti = \"C:/Users/DELL/Desktop/Kashmir/Kashmir Extracted Face\" + \"/\" + filename\n",
    "\t\t\t\t\t\tos.remove(i)\n",
    "\t\t\t\tos.rmdir(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Extracted Face\")\n",
    "\t\t\tself.extraction_path = \"C:/Users/DELL/Desktop/Kashmir/Kashmir Extracted Face\" \n",
    "\t\t\tos.mkdir(self.extraction_path)\n",
    "\t\t\ttemp = 0\n",
    "\t\t\tfor i in range(len(self.X_image)):\n",
    "\t\t\t\timg = self.X_image[i]\n",
    "\t\t\t\tresult = self.face_detector.detect_faces(img)\n",
    "\t\t\t\tk=0\n",
    "\t\t\t\tfor k in range(len(result)):\n",
    "\t\t\t\t\tif(k >= 1):\n",
    "\t\t\t\t\t\tprint(\"Required 1 person per image\")\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tx1, y1, width, height = result[k]['box']\n",
    "\t\t\t\t\t\tx1, y1 = abs(x1), abs(y1)\n",
    "\t\t\t\t\t\tx2, y2 = x1+width, y1+height\n",
    "\t\t\t\t\t\tface = img[y1:y2, x1:x2]\n",
    "\t\t\t\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\t\t\t\t\t\tface = cv2.resize(face,self.size)\n",
    "\t\t\t\t\t\tX.append(face)\n",
    "\t\t\t\t\t\ty.append(self.y_label[i])\n",
    "\t\t\t\t\t\tcv2.imwrite(self.extraction_path + \"/{}.png\".format(self.y_label[i]), face)\n",
    "\t\t\t\t\t\ttemp += 1\n",
    "\t\t\tdf = pd.DataFrame({\"Image Number\": list(range(temp)),\"Name\":y ,\"No of Upsampling\": [\"NA\"]*temp})\n",
    "\t\t\tdf.to_csv(self.extraction_path+\"/info.csv\")\n",
    "\t\t\tprint(\"Extraction Done\")\n",
    "\t\t\tprint(len(X))\n",
    "\t\t\tprint(len(y))\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"code failed in extract_face\")\n",
    "\t\t\tprint(e)\n",
    "\tdef face_augmentation(self, n = 7):\n",
    "\t\timport os\n",
    "\t\timport numpy as np\n",
    "\t\timport cv2\n",
    "\t\ttry:\n",
    "\t\t\tif(os.path.isdir(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\") == True):\n",
    "\t\t\t\tfor dirpath, dirnames, filenames in os.walk(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\"):\n",
    "\t\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\t\ti = \"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\" + \"/\" + filename\n",
    "\t\t\t\t\t\tos.remove(i)\n",
    "\t\t\t\tos.rmdir(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\")\n",
    "\t\t\tself.augmentation_path = \"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\" \n",
    "\t\t\tos.mkdir(self.augmentation_path)\n",
    "\t\t\tfor i in range(len(self.X_image)):\n",
    "\t\t\t\tself.X_image[i] = cv2.resize(self.X_image[i],(160,160))\n",
    "\t\t\tfor i in range(len(self.X_image)):\n",
    "\t\t\t\timg = self.X_image[i]\n",
    "\t\t\t\timg = np.reshape(img,(1,160,160,3))\n",
    "\t\t\t\tj=0\n",
    "\t\t\t\tfor batch in self.datagen.flow(img, batch_size=1, save_to_dir= self.augmentation_path, save_prefix = self.y_label[i], save_format = \"jpg\"):\n",
    "\t\t\t\t\tif(j >= n):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tj+=1\n",
    "\t\t\treturn self\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"Code failed in face_augmentation\")\n",
    "\t\t\tprint(e)\n",
    "\n",
    "\tdef load_augmented_image(self):\n",
    "\t\timport cv2\n",
    "\t\tX = []\n",
    "\t\ty_label = []\n",
    "\t\ttry:\n",
    "\t\t\tfor dirpath, dirnames, filenames in os.walk(\"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\"):\n",
    "\t\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\t\ti = \"C:/Users/DELL/Desktop/Kashmir/Kashmir Augmented Image\" + \"/\" + filename\n",
    "\t\t\t\t\t\timg = cv2.imread(i)\n",
    "\t\t\t\t\t\timg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\t\t\t\t\t\tX.append(img)\n",
    "\t\t\t\t\t\ty = i.split(\"/\")[-1]\n",
    "\t\t\t\t\t\ty = y.split(\"_\")\n",
    "\t\t\t\t\t\ty = y[0] + \"_\" + y[1]\n",
    "\t\t\t\t\t\ty_label.append(y)\n",
    "\t\t\tself.X_image = X\n",
    "\t\t\tself.y_label = y_label\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"Code failed in load_augmented_image\")\n",
    "\t\t\tprint(e)\n",
    "\n",
    "\tdef features_to_numbers(self,path):\n",
    "\t\tfrom keras.models import load_model\n",
    "\t\tfrom keras.utils import np_utils\n",
    "\t\timport pickle\n",
    "\n",
    "\t\tself.facenet_model = load_model(path)\n",
    "\t\tX = []\n",
    "\t\ty = []\n",
    "\t\tfor i in range(len(self.X_image)):\n",
    "\t\t\tX.append(get_embedding(self.facenet_model, self.X_image[i]))\n",
    "\t\ty = self.encoder.fit_transform(self.y_label)\n",
    "\t\tpickle.dump(self.encoder,open(\"C:/Users/DELL/Desktop/encoder\",\"wb\"))\n",
    "\t\tself.no_classes = len(set(y))\n",
    "\t\ty = np_utils.to_categorical(y, self.no_classes)\n",
    "\t\tself.X_image = X\n",
    "\t\tself.y_label = y\n",
    "\tdef training_NN(self, epochs = 10, path = r\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model\", threshold = 0.):\n",
    "\t\tfrom keras.models import Sequential\n",
    "\t\tfrom keras.layers import Dense\n",
    "\t\timport numpy as np\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\timport glob\n",
    "\n",
    "\t\tself.nn_model = Sequential()\n",
    "\t\tself.nn_model.add(Dense(self.no_classes*3, input_dim = 128 , activation = \"relu\"))\n",
    "\t\tself.nn_model.add(Dense(self.no_classes*2, activation = \"relu\"))\n",
    "\t\tself.nn_model.add(Dense(self.no_classes, activation = \"softmax\"))\n",
    "\t\tself.nn_model.compile(loss = \"categorical_crossentropy\", optimizer= \"adam\", metrics=[\"accuracy\"])\n",
    "\t\tprint(self.nn_model.summary())\n",
    "\t\tself.X_image = np.array(self.X_image)\n",
    "\t\tself.y_label = np.array(self.y_label)\n",
    "\t\thistory = self.nn_model.fit(self.X_image, self.y_label, epochs = epochs)\n",
    "\n",
    "\t\tif(history.history[\"accuracy\"][len(history.history[\"accuracy\"])-1] > threshold and history.history[\"accuracy\"][len(history.history[\"accuracy\"])-1] != 1.0):\n",
    "\t\t\tif(os.path.isdir(\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model\") == True):\n",
    "\t\t\t\tfor dirpath, dirnames, filenames in os.walk(\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model/variables\"):\n",
    "\t\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\t\ti = \"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model/variables\" + \"/\" + filename\n",
    "\t\t\t\t\t\tos.remove(i)\n",
    "\t\t\t\tos.rmdir(\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model/variables\")\n",
    "\t\t\t\tos.rmdir(\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model/assets\")\n",
    "\t\t\t\tos.remove(\"C:/Users/DELL/Desktop/Kashmir Production/new_kashmir2.model/saved_model.pb\")\n",
    "\t\t\tself.nn_model.save(path)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Kindly Check the dataset is proper or update the model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AI_Retraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_image(r\"D:/ASTRA/venv2/flask-admin-boilerplate-master/flask-admin-boilerplate-master/static/Trained_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required 1 person per image\n",
      "Extraction Done\n",
      "34\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "a.extract_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AI_Retraining at 0x15a43caffc8>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.face_augmentation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_augmented_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.features_to_numbers(\"C:/Users/DELL/Desktop/Kashmir/facenet_keras.h5/model/facenet_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 111)               14319     \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 74)                8288      \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 37)                2775      \n",
      "=================================================================\n",
      "Total params: 25,382\n",
      "Trainable params: 25,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "148/148 [==============================] - 0s 1ms/step - loss: 3.6072 - accuracy: 0.0270\n",
      "Epoch 2/20\n",
      "148/148 [==============================] - 0s 116us/step - loss: 3.4520 - accuracy: 0.1216\n",
      "Epoch 3/20\n",
      "148/148 [==============================] - 0s 132us/step - loss: 3.3303 - accuracy: 0.2027\n",
      "Epoch 4/20\n",
      "148/148 [==============================] - 0s 132us/step - loss: 3.2118 - accuracy: 0.2770\n",
      "Epoch 5/20\n",
      "148/148 [==============================] - 0s 118us/step - loss: 3.0884 - accuracy: 0.3108\n",
      "Epoch 6/20\n",
      "148/148 [==============================] - 0s 150us/step - loss: 2.9498 - accuracy: 0.3378\n",
      "Epoch 7/20\n",
      "148/148 [==============================] - 0s 141us/step - loss: 2.8069 - accuracy: 0.3986\n",
      "Epoch 8/20\n",
      "148/148 [==============================] - 0s 109us/step - loss: 2.6527 - accuracy: 0.4189\n",
      "Epoch 9/20\n",
      "148/148 [==============================] - 0s 119us/step - loss: 2.4868 - accuracy: 0.5676\n",
      "Epoch 10/20\n",
      "148/148 [==============================] - 0s 102us/step - loss: 2.3159 - accuracy: 0.6284\n",
      "Epoch 11/20\n",
      "148/148 [==============================] - 0s 101us/step - loss: 2.1347 - accuracy: 0.6892\n",
      "Epoch 12/20\n",
      "148/148 [==============================] - 0s 101us/step - loss: 1.9539 - accuracy: 0.6824\n",
      "Epoch 13/20\n",
      "148/148 [==============================] - 0s 104us/step - loss: 1.7686 - accuracy: 0.7365\n",
      "Epoch 14/20\n",
      "148/148 [==============================] - 0s 128us/step - loss: 1.5978 - accuracy: 0.7365\n",
      "Epoch 15/20\n",
      "148/148 [==============================] - 0s 114us/step - loss: 1.4292 - accuracy: 0.7905\n",
      "Epoch 16/20\n",
      "148/148 [==============================] - 0s 88us/step - loss: 1.2772 - accuracy: 0.8176\n",
      "Epoch 17/20\n",
      "148/148 [==============================] - 0s 162us/step - loss: 1.1325 - accuracy: 0.8649\n",
      "Epoch 18/20\n",
      "148/148 [==============================] - 0s 88us/step - loss: 1.0041 - accuracy: 0.8986\n",
      "Epoch 19/20\n",
      "148/148 [==============================] - 0s 101us/step - loss: 0.8910 - accuracy: 0.9054\n",
      "Epoch 20/20\n",
      "148/148 [==============================] - 0s 134us/step - loss: 0.7874 - accuracy: 0.9392\n"
     ]
    }
   ],
   "source": [
    "a.training_NN(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
